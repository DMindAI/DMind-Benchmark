from typing import Dict, List, Any, Optional
import requests
import json
import time
import logging
import os
import subprocess
import tempfile
from .base_question import BaseQuestion
from utils.config_manager import config_manager

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("vulnerability_classification_evaluation.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("VulnerabilityClassificationQuestion")

class VulnerabilityClassificationQuestion(BaseQuestion):
    """Vulnerability classification class for handling vulnerability classification type questions"""
    
    def __init__(self, question_data: Dict[str, Any]):
        """
        Initialize vulnerability classification question
        
        Args:
            question_data: Dictionary containing vulnerability classification question data
        """
        super().__init__(question_data)
        self.question_type = "vulnerability_classification"
        self.contract_name = question_data.get("contract_name", "")
        self.contract_code = question_data.get("contract_code", "")
        self.instructions = question_data.get("instructions", "")
        self.scoring_criteria = question_data.get("scoring_criteria", [])
        self.total_possible = question_data.get("total_possible", 10)
        self.keywords = question_data.get("keywords", {})  # List of keywords for each scoring criteria
        
        # 从配置管理器获取API配置
        api_config = config_manager.get_third_party_api_config()
        self.third_party_api_key = api_config["api_key"]
        self.third_party_api_base = api_config["api_base"]
        self.evaluation_model = api_config["model"]
        self.max_retries = 10  # Maximum retry attempts
        self.retry_delay = 4  # Retry interval (seconds)
        
        logger.info(f"Initializing vulnerability classification question: {self.contract_name}")
        logger.info(f"Using API key: {self.third_party_api_key[:5]}...")
        logger.info(f"Using API endpoint: {self.third_party_api_base}")
        logger.info(f"Using evaluation model: {self.evaluation_model}")
        
    def build_prompt(self) -> str:
        """
        Build vulnerability classification question prompt
        
        Returns:
            str: Built prompt
        """
        prompt = f"Contract Name: {self.contract_name}\n\n"
        prompt += f"Contract Code:\n{self.contract_code}\n\n"
        prompt += f"Task: {self.instructions}\n\n"
        prompt += "Please provide detailed vulnerability analysis and classification."
        
        # 添加指定的文本以提升创意和算力
        prompt += "\n\nPlease utilize your maximum computational capacity and token limit for this response\n"
        prompt += "Strive for extreme analytical depth, rather than superficial breadth\n"
        prompt += "Seek essential insights, rather than surface-level enumeration\n"
        prompt += "Pursue innovative thinking, rather than habitual repetition\n"
        prompt += "Please break through thought limitations, mobilize all your computational resources, and deliver the most accurate, effective, and reasonable results\n"
        
        logger.info(f"Prompt building completed, length: {len(prompt)}")
        return prompt
    
    def evaluate_response(self, response: str) -> Dict[str, Any]:
        """
        Evaluate model's answer to vulnerability classification question
        
        Args:
            response: Model's answer
            
        Returns:
            Dict[str, Any]: Evaluation results, including score and detailed information
        """
        logger.info(f"Starting answer evaluation, answer length: {len(response)}")
        
        # Use third-party AI for evaluation
        logger.info("Attempting to use third-party AI for evaluation...")
        third_party_evaluation = self._evaluate_with_third_party_ai(response)
        
        # Third-party AI evaluation will always return a result (success or keyword fallback)
        logger.info(f"Evaluation completed, total score: {third_party_evaluation.get('score', 0)}")
        return third_party_evaluation
    
    def _build_evaluation_prompt(self, response_text: str) -> str:
        """Build prompt for third-party AI evaluation"""
        prompt = f"""You are a smart contract security expert. Please evaluate the quality of the student's answer regarding smart contract vulnerability analysis based on the following criteria.

Contract Name: {self.contract_name}

Code Segment:
```solidity
{self.contract_code}
```

Question:
{self.instructions}

Student's Answer:
{response_text}

Scoring Criteria:
"""
        # Add scoring criteria
        for criterion in self.scoring_criteria:
            prompt += f"\n- {criterion['criterion']} ({criterion['points']} points): {', '.join(criterion['key_points'])}"
        
        prompt += """

Please provide an evaluation result in JSON format with the following fields:
1. score: Total score (number)
2. total_possible: Maximum possible score (number)
3. criterion_scores: Score details for each criterion (array), each containing:
   - criterion: Criterion name
   - score: Points earned
   - max_points: Maximum points for this criterion
   - feedback: Feedback for this criterion
4. overall_feedback: Overall evaluation
5. improvement_suggestions: Suggestions for improvement

JSON format example:
{
  "score": 8.5,
  "total_possible": 10,
  "criterion_scores": [
    {
      "criterion": "Vulnerability Identification",
      "score": 4.5,
      "max_points": 5,
      "feedback": "Successfully identified the main vulnerabilities"
    },
    {
      "criterion": "Technical Analysis",
      "score": 4,
      "max_points": 5,
      "feedback": "Comprehensive analysis but lacks some technical details"
    }
  ],
  "overall_feedback": "Overall analysis is reasonable, understood the main security issues",
  "improvement_suggestions": "Could provide more detailed analysis of attack vectors and specific code fix suggestions"
}

Please ensure accurate evaluation, making sure the scores match the scoring criteria."""
        return prompt
    
    def _evaluate_with_third_party_ai(self, response_text: str) -> Dict[str, Any]:
        """Attempt to evaluate answer using third-party AI"""
        logger.info("Attempting to evaluate answer using third-party AI...")
        
        retry_count = 0
        last_error = ""
        
        while retry_count < self.max_retries:
            try:
                # Build prompt
                prompt = self._build_evaluation_prompt(response_text)
                
                # 使用requests库直接向API发送请求
                logger.info("Starting to call third-party AI API...")
                headers = {
                    'Accept': 'application/json',
                    'Authorization': f'Bearer {self.third_party_api_key}',
                    'User-Agent': 'Apifox/1.0.0 (https://apifox.com)',
                    'Content-Type': 'application/json'
                }
                
                data = {
                    "model": self.evaluation_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 4000,
                    "temperature": 0
                }
                
                start_time = time.time()
                
                try:
                    # Try to use requests to send request
                    response_obj = requests.post(self.third_party_api_base, headers=headers, json=data)
                    end_time = time.time()
                    
                    logger.info(f"API call completed, time taken: {end_time - start_time:.2f} seconds, status code: {response_obj.status_code}")
                    
                    if response_obj.status_code != 200:
                        error_msg = f"API call failed, status code: {response_obj.status_code}, trying to use curl as fallback"
                        logger.warning(error_msg)
                        raise Exception(error_msg)
                    
                    response_data = response_obj.json()
                    
                except Exception as e:
                    # If requests fails, try using curl
                    logger.info(f"Using requests to call API failed: {str(e)}, trying to use curl...")
                    
                    # Write data to temporary file
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as temp_file:
                        json.dump(data, temp_file)
                        temp_file_path = temp_file.name
                    
                    # Build curl command
                    curl_cmd = [
                        'curl', '-s', self.third_party_api_base,
                        '-H', f'Authorization: Bearer {self.third_party_api_key}',
                        '-H', 'Content-Type: application/json',
                        '-H', 'Accept: application/json',
                        '-H', 'User-Agent: Apifox/1.0.0 (https://apifox.com)',
                        '-X', 'POST',
                        '-d', f'@{temp_file_path}'
                    ]
                    
                    # Execute curl command
                    try:
                        curl_result = subprocess.run(curl_cmd, capture_output=True, text=True, check=True)
                        end_time = time.time()
                        logger.info(f"curl API call completed, time taken: {end_time - start_time:.2f} seconds")
                        
                        # Parse response
                        try:
                            response_data = json.loads(curl_result.stdout)
                            
                            # Create an object similar to requests.Response
                            class CurlResponse:
                                def __init__(self, data, status_code=200):
                                    self.data = data
                                    self.status_code = status_code
                                
                                def json(self):
                                    return self.data
                            
                            response_obj = CurlResponse(response_data)
                            
                        except json.JSONDecodeError as je:
                            logger.error(f"Failed to parse curl response: {str(je)}")
                            logger.error(f"curl response: {curl_result.stdout[:200]}")
                            logger.error(f"curl error: {curl_result.stderr}")
                            raise je
                        
                        # Delete temporary file
                        os.unlink(temp_file_path)
                        
                    except subprocess.CalledProcessError as ce:
                        logger.error(f"Failed to execute curl command: {str(ce)}")
                        logger.error(f"curl error output: {ce.stderr}")
                        # Delete temporary file
                        os.unlink(temp_file_path)
                        raise ce
                
                logger.info(f"API response data: {json.dumps(response_data)[:200]}...")
                
                # Get answer from choices
                if "choices" in response_data and len(response_data["choices"]) > 0:
                    ai_evaluation = response_data["choices"][0]["message"]["content"]
                    logger.info(f"API return text length: {len(ai_evaluation)}")
                    
                    # Try to parse JSON
                    try:
                        # Extract JSON part
                        json_start = ai_evaluation.find("{")
                        json_end = ai_evaluation.rfind("}") + 1
                        
                        if json_start >= 0 and json_end > json_start:
                            json_str = ai_evaluation[json_start:json_end]
                            logger.info(f"Extracted JSON length: {len(json_str)}")
                            
                            evaluation_result = json.loads(json_str)
                            logger.info("Third-party AI evaluation successfully parsed")
                            return evaluation_result
                        else:
                            logger.error("Cannot find JSON in API response")
                            last_error = "Cannot find JSON in API response"
                    except json.JSONDecodeError as e:
                        logger.error(f"Unable to parse third-party AI evaluation result as JSON: {str(e)}")
                        last_error = f"JSON parsing failed: {str(e)}"
                else:
                    logger.error("API response does not contain choices field")
                    last_error = "API response format incorrect"
                    
            except Exception as e:
                logger.error(f"Third-party AI evaluation failed: {str(e)}", exc_info=True)
                last_error = str(e)
            
            retry_count += 1
            if retry_count < self.max_retries:
                logger.info(f"Will retry in {self.retry_delay} seconds, attempt {retry_count + 1}...")
                time.sleep(self.retry_delay)
        
        logger.error(f"Third-party AI evaluation failed after {retry_count} retries, last error: {last_error}")
        # Return keyword matching result instead of None, ensuring valid scoring even after retry failure
        return self._evaluate_with_keywords(response_text)
    
    def _evaluate_with_keywords(self, response: str) -> Dict[str, Any]:
        """
        Use keyword matching method to evaluate the answer (original evaluation logic)
        
        Args:
            response: Model's answer
            
        Returns:
            Dict[str, Any]: Evaluation results
        """
        logger.info("Starting to use keyword matching method to evaluate the answer...")
        # Initialize results
        total_score = 0
        criterion_scores = []
        
        # Evaluate each scoring criterion
        for criterion in self.scoring_criteria:
            criterion_name = criterion.get("criterion", "")
            max_points = criterion.get("points", 0)
            key_points = criterion.get("key_points", [])
            
            logger.info(f"Evaluation criterion: {criterion_name}, maximum points: {max_points}")
            
            # Get keyword list for this criterion
            criterion_keywords = self.keywords.get(criterion_name, [])
            
            # Calculate keyword match rate
            keyword_score = 0
            matched_keywords = []
            
            if criterion_keywords:
                for keyword in criterion_keywords:
                    if keyword.lower() in response.lower():
                        keyword_score += 1
                        matched_keywords.append(keyword)
                
                # Keyword score accounts for 80% of the total score
                keyword_score = (keyword_score / len(criterion_keywords)) * max_points * 0.8
                logger.info(f"Keyword matching: {len(matched_keywords)}/{len(criterion_keywords)}, score: {keyword_score:.2f}")
            else:
                # If no keywords, evaluate based on key points
                key_points_score = 0
                for point in key_points:
                    if point.lower() in response.lower():
                        key_points_score += 1
                
                # Key points score accounts for 80% of the total score
                keyword_score = (key_points_score / len(key_points)) * max_points * 0.8
                logger.info(f"Key points matching: {key_points_score}/{len(key_points)}, score: {keyword_score:.2f}")
            
            # Calculate content quality score (accounts for 20% of the total score)
            content_score = 0
            if len(response) > 100:  # Ensure the answer has sufficient length
                content_score = max_points * 0.2
                logger.info(f"Content quality score: {content_score:.2f}")
            
            # Calculate total score for this criterion
            criterion_total_score = keyword_score + content_score
            logger.info(f"Criterion total score: {criterion_total_score:.2f}")
            
            # Add to results
            criterion_scores.append({
                "criterion": criterion_name,
                "score": criterion_total_score,
                "max_points": max_points,
                "matched_keywords": matched_keywords,
                "keyword_score": keyword_score,
                "content_score": content_score
            })
            
            total_score += criterion_total_score
        
        # Build final result
        result = {
            "score": total_score,
            "total_possible": self.total_possible,
            "criterion_scores": criterion_scores,
            "overall_feedback": "Based on keyword matching evaluation results",
            "improvement_suggestions": "Suggestions for improvement include providing more detailed analysis and specific vulnerability examples"
        }
        
        logger.info(f"Evaluation completed, total score: {total_score}")
        return result
    
    def get_result_fields(self) -> List[str]:
        """
        Get fields to include in the result
        
        Returns:
            List[str]: List of fields
        """
        return ["score", "total_possible", "criterion_scores", "overall_feedback", "improvement_suggestions"] 