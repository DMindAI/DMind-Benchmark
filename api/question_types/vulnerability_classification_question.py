from typing import Dict, List, Any, Optional
import requests
import json
import time
import logging
import os
from .base_question import BaseQuestion
import openai

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("vulnerability_classification_evaluation.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("VulnerabilityClassificationQuestion")

class VulnerabilityClassificationQuestion(BaseQuestion):
    """漏洞分类类，用于处理漏洞分类类型的题目"""
    
    def __init__(self, question_data: Dict[str, Any]):
        """
        初始化漏洞分类题
        
        Args:
            question_data: 包含漏洞分类题数据的字典
        """
        super().__init__(question_data)
        self.question_type = "vulnerability_classification"
        self.contract_name = question_data.get("contract_name", "")
        self.contract_code = question_data.get("contract_code", "")
        self.instructions = question_data.get("instructions", "")
        self.scoring_criteria = question_data.get("scoring_criteria", [])
        self.total_possible = question_data.get("total_possible", 10)
        self.keywords = question_data.get("keywords", {})  # 每个评分标准的关键词列表
        
        # 从环境变量获取API密钥，如果不存在则使用默认值
        self.third_party_api_key = os.environ.get("CLAUDE_API_KEY", "sk-sjkpMQ7WsWk5jUShcqhK4RSe3GEooupy8jsy7xQkbg6eQaaX")
        self.third_party_api_base = "https://api.claude-plus.top/v1/chat/completions"
        self.max_retries = 10  # 最大重试次数
        self.retry_delay = 4  # 重试间隔（秒）
        self.evaluation_model = "claude-3-7-sonnet-20250219"
        logger.info(f"初始化漏洞分类题: {self.contract_name}")
        logger.info(f"使用API密钥: {self.third_party_api_key[:5]}...")
        
    def build_prompt(self) -> str:
        """
        构建漏洞分类题的提示
        
        Returns:
            str: 构建好的提示
        """
        prompt = f"合约名称：{self.contract_name}\n\n"
        prompt += f"合约代码：\n{self.contract_code}\n\n"
        prompt += f"任务：{self.instructions}\n\n"
        prompt += "请提供详细的漏洞分析和分类。"
        logger.info(f"构建提示完成，长度: {len(prompt)}")
        return prompt
    
    def evaluate_response(self, response: str) -> Dict[str, Any]:
        """
        评估模型对漏洞分类题的回答
        
        Args:
            response: 模型的回答
            
        Returns:
            Dict[str, Any]: 评估结果，包含分数和详细信息
        """
        logger.info(f"开始评估回答，回答长度: {len(response)}")
        
        # 使用第三方AI进行评测
        logger.info("尝试使用第三方AI进行评测...")
        third_party_evaluation = self._evaluate_with_third_party_ai(response)
        
        # 第三方AI评测总会返回结果（成功或关键词备用方案）
        logger.info(f"评测完成，总分: {third_party_evaluation.get('score', 0)}")
        return third_party_evaluation
    
    def _build_evaluation_prompt(self, response_text: str) -> str:
        """构建用于第三方AI评估的提示"""
        prompt = f"""你是一个智能合约安全专家。请按照以下标准评估学生对智能合约漏洞分析的回答质量。

合约名称: {self.contract_name}

代码段:
```solidity
{self.contract_code}
```

问题:
{self.instructions}

学生回答:
{response_text}

评分标准:
"""
        # 添加评分标准
        for criterion in self.scoring_criteria:
            prompt += f"\n- {criterion['criterion']} ({criterion['points']}分): {', '.join(criterion['key_points'])}"
        
        prompt += """

请提供一个JSON格式的评估结果，包含以下字段:
1. score: 总得分(数字)
2. total_possible: 总分值(数字)
3. criterion_scores: 各评分标准的得分详情(数组)，每项包含:
   - criterion: 标准名称
   - score: 得分
   - max_points: 该标准满分
   - feedback: 针对该标准的反馈
4. overall_feedback: 整体评价
5. improvement_suggestions: 改进建议

JSON格式示例:
{
  "score": 8.5,
  "total_possible": 10,
  "criterion_scores": [
    {
      "criterion": "漏洞识别",
      "score": 4.5,
      "max_points": 5,
      "feedback": "成功识别了主要漏洞"
    },
    {
      "criterion": "技术分析",
      "score": 4,
      "max_points": 5,
      "feedback": "分析全面但缺少一些技术细节"
    }
  ],
  "overall_feedback": "整体分析合理，理解了主要安全问题",
  "improvement_suggestions": "可以更详细分析攻击向量和提供具体代码修复建议"
}

请务必准确评估，确保评分与评分标准匹配。"""
        return prompt
    
    def _evaluate_with_third_party_ai(self, response_text: str) -> Dict[str, Any]:
        """尝试使用第三方AI评估回答"""
        logger.info("尝试使用第三方AI评估回答...")
        
        retry_count = 0
        last_error = ""
        
        while retry_count < self.max_retries:
            try:
                # 构建提示
                prompt = self._build_evaluation_prompt(response_text)
                
                # 向第三方AI发送请求
                api_response = openai.ChatCompletion.create(
                    model=self.evaluation_model,
                    messages=[
                        {"role": "system", "content": "你是一个专业的智能合约安全评估助手。"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.1
                )
                
                # 提取回答
                ai_evaluation = api_response['choices'][0]['message']['content']
                
                # 尝试解析JSON
                try:
                    evaluation_result = json.loads(ai_evaluation)
                    logger.info("第三方AI评估成功解析")
                    return evaluation_result
                except json.JSONDecodeError:
                    logger.error(f"无法解析第三方AI评估结果为JSON: {ai_evaluation}")
                    # 解析失败时返回关键词匹配的结果
                    return self._evaluate_with_keywords(response_text)
                
            except Exception as e:
                logger.error(f"第三方AI评测失败: {str(e)}", exc_info=True)
                last_error = str(e)
            
            retry_count += 1
            if retry_count < self.max_retries:
                logger.info(f"将在 {self.retry_delay} 秒后进行第 {retry_count + 1} 次重试...")
                time.sleep(self.retry_delay)
        
        logger.error(f"第三方AI评测失败，已重试 {retry_count} 次，最后一次错误: {last_error}")
        # 返回关键词匹配的结果，而不是None，确保重试失败后仍能返回有效评分
        return self._evaluate_with_keywords(response_text)
    
    def _evaluate_with_keywords(self, response: str) -> Dict[str, Any]:
        """
        使用关键词匹配方法评估回答（原有评测逻辑）
        
        Args:
            response: 模型的回答
            
        Returns:
            Dict[str, Any]: 评估结果
        """
        logger.info("开始使用关键词匹配方法评估回答...")
        # 初始化结果
        total_score = 0
        criterion_scores = []
        
        # 对每个评分标准进行评估
        for criterion in self.scoring_criteria:
            criterion_name = criterion.get("criterion", "")
            max_points = criterion.get("points", 0)
            key_points = criterion.get("key_points", [])
            
            logger.info(f"评估标准: {criterion_name}, 满分: {max_points}")
            
            # 获取该标准的关键词列表
            criterion_keywords = self.keywords.get(criterion_name, [])
            
            # 计算关键词匹配度
            keyword_score = 0
            matched_keywords = []
            
            if criterion_keywords:
                for keyword in criterion_keywords:
                    if keyword.lower() in response.lower():
                        keyword_score += 1
                        matched_keywords.append(keyword)
                
                # 关键词得分占总分的80%
                keyword_score = (keyword_score / len(criterion_keywords)) * max_points * 0.8
                logger.info(f"关键词匹配: {len(matched_keywords)}/{len(criterion_keywords)}, 得分: {keyword_score:.2f}")
            else:
                # 如果没有关键词，则基于关键点评估
                key_points_score = 0
                for point in key_points:
                    if point.lower() in response.lower():
                        key_points_score += 1
                
                # 关键点得分占总分的80%
                keyword_score = (key_points_score / len(key_points)) * max_points * 0.8
                logger.info(f"关键点匹配: {key_points_score}/{len(key_points)}, 得分: {keyword_score:.2f}")
            
            # 计算内容质量得分（占总分的20%）
            content_score = 0
            if len(response) > 100:  # 确保回答有足够的长度
                content_score = max_points * 0.2
                logger.info(f"内容质量得分: {content_score:.2f}")
            
            # 计算该标准的总分
            criterion_total_score = keyword_score + content_score
            logger.info(f"标准总分: {criterion_total_score:.2f}")
            
            # 添加到结果中
            criterion_scores.append({
                "criterion": criterion_name,
                "score": criterion_total_score,
                "max_points": max_points,
                "matched_keywords": matched_keywords,
                "keyword_score": keyword_score,
                "content_score": content_score
            })
            
            total_score += criterion_total_score
        
        # 构建最终结果
        result = {
            "score": total_score,
            "total_possible": self.total_possible,
            "criterion_scores": criterion_scores,
            "overall_feedback": "基于关键词匹配的评估结果",
            "improvement_suggestions": "建议提供更详细的分析和具体的漏洞示例"
        }
        
        logger.info(f"评估完成，总分: {total_score}")
        return result
    
    def get_result_fields(self) -> List[str]:
        """
        获取结果中需要包含的字段
        
        Returns:
            List[str]: 字段列表
        """
        return ["score", "total_possible", "criterion_scores", "overall_feedback", "improvement_suggestions"] 