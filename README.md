---
configs:
- config_name: subjective
  data_files:
  - split: DAO
    path:
    - "test_data/subjective/DAO.jsonl"
  - split: Fundamentals
    path:
    - "test_data/subjective/Blockchain_Foundamantals_benchmark.jsonl"
  - split: Defi
    path:
    - "test_data/subjective/Defi.jsonl"
  - split: Infrastructure
    path:
    - "test_data/subjective/infra.jsonl"
  - split: MEME
    path:
    - "test_data/subjective/MEME.jsonl"
  
---

# ğŸ” DMind Benchmark
A comprehensive framework for evaluating large language models (LLMs) on blockchain, cryptocurrency, and Web3 knowledge across multiple domains.

## ğŸ“Š Overview

This project provides tools to benchmark AI models on their understanding of blockchain concepts through both objective (multiple-choice) and subjective (open-ended) questions. The framework covers various domains including:

- ğŸ§± Blockchain Fundamentals
- ğŸ’° DeFi (Decentralized Finance)
- ğŸ“ Smart Contracts
- ğŸ›ï¸ DAOs (Decentralized Autonomous Organizations)
- ğŸ–¼ï¸ NFTs
- ğŸ”’ Security
- ğŸ’¹ Tokenomics
- ğŸ­ MEME coins
- ğŸŒ Blockchain Infrastructure

## âœ¨ Features

- ğŸ§ª Test models on multiple-choice questions with single or multiple correct answers
- ğŸ“‹ Evaluate models on open-ended questions requiring detailed explanations
- ğŸ”„ Support for various question types including:
  - ğŸ“Š Calculation questions
  - ğŸ” Code audit questions
  - ğŸ“ Fill-in-blank questions
  - ğŸ“ˆ Market reasoning questions
  - ğŸ”— Matching questions
  - ğŸ“‹ Ordering questions
  - âš ï¸ Risk analysis questions
  - ğŸ”® Scenario analysis questions
  - âœï¸ Short answer questions
  - ğŸ§© Strategy analysis questions
  - ğŸ›¡ï¸ Vulnerability classification questions
- ğŸ¤– Automated scoring and evaluation
- ğŸ“Š Calculate total scores and comparative analysis across models

## ğŸ› ï¸ Installation

1. Install the required packages:

   ```bash
   pip install -r requirements.txt
   ```

2. Configure your API settings in models.yml:

   ```bash
   api_base: "your_api_base"
   # Add other configuration settings as needed
   ```

## ğŸ“‹ Usage

The project includes a Makefile with commands to run different tests:

```bash
# Run objective tests
make test-objective model=gpt-4o

# Run subjective tests
make test-subjective model=gpt-4o

# Calculate total score for a specific model
make calculate-model model=gpt-4o
```

### Testing Multiple Models

You can test multiple models and compare their performance:

1. Add models to your models.yml configuration
2. Run tests for each model
3. Use the calculation tools to compare results

## ğŸ“ Project Structure

```
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ calculate_total_score.py  # Calculate and analyze model scores
â”‚   â”œâ”€â”€ test_objective.py         # Run objective tests (multiple choice)
â”‚   â”œâ”€â”€ test_subjective.py        # Run subjective tests (open-ended)
â”‚   â””â”€â”€ question_types/           # Question types implementation
â”‚       â”œâ”€â”€ base_question.py      # Base class for all question types
â”‚       â”œâ”€â”€ calculation_question.py
â”‚       â”œâ”€â”€ code_audit_question.py
â”‚       â””â”€â”€ ...
â”œâ”€â”€ test_data/
â”‚   â”œâ”€â”€ objective/                # Multiple choice questions in CSV format
â”‚   â”‚   â”œâ”€â”€ Blockchain_Fundamentals_benchmark.csv
â”‚   â”‚   â”œâ”€â”€ DAO2.csv
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ subjective/               # Open-ended questions in JSON format
â”‚       â”œâ”€â”€ Blockchain_Fundamentals_benchmark.json
â”‚       â”œâ”€â”€ DAO.json
â”‚       â””â”€â”€ ...
â”œâ”€â”€ models.yml                    # Model configuration
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ Makefile                      # Commands for running tests
```

## ğŸ“ Evaluation Methodology

The evaluation framework uses various techniques to assess model performance:

- For objective questions: Exact matching against correct answers
- For subjective questions: Combination of keyword analysis, structured evaluation, and third-party AI evaluation when configured

## âš™ï¸ Customization

- Add new questions by extending the CSV/JSON files in test_data/
- Implement new question types by extending the BaseQuestion class
- Configure evaluation parameters in the respective question type implementations